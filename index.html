<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">
    <title>Litao Liu Bio</title>
    <meta name="author" content="Litao Liu">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Litao Liu
                </p>
                <p>
                  <a style="font-weight: bold; color: red;">I am actively seeking a Ph.D. position for Fall 2025!</a> My research interests lie at the
                  intersection of Artificial Intelligence and Robotics, including but not limited to Robot Learning,
                  Robotic Manipulation, Learning-based Control, Autonomous Vehicles, Intelligent Systems,
                  and Multi-agent Systems.
                </p>
                <p>
                  I received my <a style="font-weight: bold; color: black">Bachelor of Engineering</a> degree from <a href="https://en.scu.edu.cn/" target="_blank">Sichuan University</a> in 2023. During my undergraduate studies, I had the privilege of working with many outstanding professors.
                  As the first in my family to attend college, I owe my rapid progress to their invaluable guidance and support.
                  They convinced me of the value of deep research and the meaning of dedicated teaching.
                </p>
                <p>
                  After graduation, I spent about six months founding a graduate admissions consulting company,
                  dedicated to helping college students, particularly first-generation college students,
                  bridge the information gap and successfully pursue higher education. Subsequently,
                  I began preparing my own graduate applications and was fortunate to become a research intern in Robot
                  Learning with <a href="https://lianwenzhao.github.io/" target="_blank">Professor Lian</a>, successfully entering the field of intelligent robotics, which I have
                  long been passionate about.
                </p>

                <p style="text-align:center">
                  <a href="mailto:liulitao011026@gmail.com">Email</a> &nbsp;/&nbsp;
                  <a href="data/CV_LitaoLIU.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=r_CvWNYAAAAJ&hl=en&authuser=1">Google Scholar</a> &nbsp;/&nbsp;
                </p>
              </td>
              <td style="padding:2.5%;width:37%;max-width:37%">
                <a><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/LitaoLiu.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Selected Publications</h2>
                <p>
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:16px;width:20%;vertical-align:middle">
              <div class="one">
                <div class="two">
                  <video width="100%" height="100%" muted autoplay loop>
                    <source src="images/FoAM.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                  </video>
                </div>
              </div>
            </td>
            <td style="padding:8px;width:80%;vertical-align:middle">
              <a href="https://m-niemeyer.github.io/regnerf/index.html">
                <span class="papertitle">FoAM: Foresight-Augmented Multi-Task Imitation Policy for Robotic Manipulation</span>
              </a>
              <br>
              <strong>Litao Liu</strong>,
              <span>Wentao Wang</span>,
              <span>Yifan Han</span>,
              <span>Zhuoli Xie</span>,
              <span>Pengfei Yi</span>,
              <span>Junyan Li</span>,
              <span>Yi Qin</span>,
              <span>Wenzhao Lian</span>
              <br>
              <em>arXiv preprint</em>
              <br>
              <a href="https://projfoam.github.io/">project page</a> /
              <a href="https://arxiv.org/abs/2409.19528">arXiv</a>
              <br>
              <p>We developed the FoAM, a multi-task imitation learning framework that integrates a vision-language model to autonomously acquire multi-goal conditions from language prompts and visual inputs. We introduced the Foresight-Augmented module and open-sourced the FoAM benchmark that has over 80 tasks across 10 suites and a dataset of 14 real-world tasks. Evaluations across 100+ tasks showed up to a 41% improvement in success rates over state-of-the-art baselines.</p>
            </td>
          </tr>

<!--          <tr onmouseout="regnerf_stop()" onmouseover="regnerf_start()">-->
<!--            <td style="padding:16px;width:20%;vertical-align:middle">-->
<!--              <div class="one">-->
<!--                <div class="two" id='regnerf_image'><video  width=100% height=100% muted autoplay loop>-->
<!--                <source src="images/regnerf_after.mp4" type="video/mp4">-->
<!--                Your browser does not support the video tag.-->
<!--                </video></div>-->
<!--                <img src='images/regnerf_before.jpeg' width="160">-->
<!--              </div>-->
<!--              <script type="text/javascript">-->
<!--                function regnerf_start() {-->
<!--                  document.getElementById('regnerf_image').style.opacity = "1";-->
<!--                }-->

<!--                function regnerf_stop() {-->
<!--                  document.getElementById('regnerf_image').style.opacity = "0";-->
<!--                }-->
<!--                regnerf_stop()-->
<!--              </script>-->
<!--            </td>-->
<!--            <td style="padding:8px;width:80%;vertical-align:middle">-->
<!--              <a href="https://m-niemeyer.github.io/regnerf/index.html">-->
<!--                <span class="papertitle">FoAM: Foresight-Augmented Multi-Task Imitation Policy for Robotic Manipulation</span>-->
<!--              </a>-->
<!--              <br>-->
<!--              <strong>Litao Liu</strong>,-->
<!--              <span>Wentao Wang</span>,-->
<!--              <span>Yifan Han</span>,-->
<!--              <span>Zhuoli Xie</span>,-->
<!--              <span>Pengfei Yi</span>,-->
<!--              <span>Junyan Li</span>,-->
<!--              <span>Yi Qin</span>,-->
<!--              <span>Wenzhao Lian</span>-->
<!--              <br>-->
<!--        <em>arXiv preprint</em>-->
<!--              <br>-->
<!--              <a href="https://projfoam.github.io/">project page</a>-->
<!--        /-->
<!--              <a href="https://arxiv.org/abs/2409.19528">arXiv</a>-->
<!--        /-->
<!--              <p></p>-->
<!--              <p>We developed the FoAM, a multi-task imitation learning framework that integrates a-->
<!--                vision-language model to autonomously acquire multi-goal conditions from language prompts and visual inputs.-->
<!--                We introduced the Foresight-Augmented module, which improves task activation by learning the differences between visual-->
<!--                observations and goal images. We also created and open-sourced the FoAM benchmark, with over 80 tasks-->
<!--                across 10 suites and a dataset of 14 real-world tasks. Evaluations across 100+ tasks showed up to a-->
<!--                41% improvement in success rates over state-of-the-art baselines.</p>-->
<!--            </td>-->
<!--          </tr>-->



          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                  <p style="text-align: center; font-size: small;">
                    Design and source code from <a href="https://github.com/jonbarron/jonbarron_website" target="_blank" rel="noopener noreferrer">Jon Barron's website</a>.
                  </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
